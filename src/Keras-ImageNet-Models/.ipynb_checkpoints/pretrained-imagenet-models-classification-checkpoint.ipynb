{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models from keras applications folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      " 52002816/553467096 [=>............................] - ETA: 16:03"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "\n",
    "vgg_model = vgg16.VGG16(weights='imagenet')\n",
    "\n",
    "inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    "\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required image preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'images/cat.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Pre-processing before feeding the image to the network\n",
    "\n",
    "#### We perform the following pre-processing to the image\n",
    "\n",
    "1. Keras loads the image using PIL library. This is done using the **load_img** function. The image is in width x height x channels format.\n",
    "2. Convert the image from PIL format to Numpy format ( height x width x channels ) using **image_to_array** function.\n",
    "3. Form a batch of image( s ) to feed the network. This is done using the **expand_dims** function in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image in PIL format\n",
    "original = load_img(filename, target_size=(224, 224))\n",
    "print('PIL image size',original.size)\n",
    "plt.imshow(original)\n",
    "plt.show()\n",
    "\n",
    "# convert the PIL image to a numpy array\n",
    "# IN PIL - image is in (width, height, channel)\n",
    "# In Numpy - image is in (height, width, channel)\n",
    "numpy_image = img_to_array(original)\n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "plt.show()\n",
    "print('numpy array size',numpy_image.shape)\n",
    "\n",
    "# Convert the image / images into batch format\n",
    "# expand_dims will add an extra dimension to the data at a particular axis\n",
    "# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "# Thus we add the extra dimension to the axis 0.\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "print('image batch size', image_batch.shape)\n",
    "plt.imshow(np.uint8(image_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions using the various Network\n",
    "\n",
    "1. Preprocess the input by subtracting the mean value from each channel of the images in the batch. Mean is an array of three elements obtained by the average of R, G, B pixels of all images obtained from ImageNet\n",
    "2. get the probabilities of occurrence for each class\n",
    "3. convert the probabilities to human-readable labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the image for the VGG model\n",
    "processed_image = vgg16.preprocess_input(image_batch.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions = vgg_model.predict(processed_image)\n",
    "# print predictions\n",
    "# convert the probabilities to class labels\n",
    "# We will get top 5 predictions which is the default\n",
    "label_vgg = decode_predictions(predictions)\n",
    "label_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the image for the ResNet50 model\n",
    "processed_image = resnet50.preprocess_input(image_batch.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions = resnet_model.predict(processed_image)\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "# If you want to see the top 3 predictions, specify it using the top argument\n",
    "label_resnet = decode_predictions(predictions, top=3)\n",
    "label_resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the image for the MobileNet model\n",
    "processed_image = mobilenet.preprocess_input(image_batch.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions = mobilenet_model.predict(processed_image)\n",
    "\n",
    "# convert the probabilities to imagenet class labels\n",
    "label_mobilenet = decode_predictions(predictions)\n",
    "label_mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception_V3 Network\n",
    "\n",
    "+ The input size for inception network is different from the other networks. It accepts inputs of size (299, 299).\n",
    "+ Thus we load the image with target size according to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image in PIL format\n",
    "original = load_img(filename, target_size=(299, 299))\n",
    "\n",
    "# Convert the PIL image into numpy array\n",
    "numpy_image = img_to_array(original)\n",
    "\n",
    "# reshape data in terms of batchsize\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "\n",
    "# prepare the image for the Inception model\n",
    "processed_image = inception_v3.preprocess_input(image_batch.copy())\n",
    "\n",
    "# get the predicted probabilities for each class\n",
    "predictions = inception_model.predict(processed_image)\n",
    "\n",
    "# convert the probabilities to class labels\n",
    "label_inception = decode_predictions(predictions)\n",
    "label_inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "numpy_image = np.uint8(img_to_array(original)).copy()\n",
    "numpy_image = cv2.resize(numpy_image,(900,900))\n",
    "\n",
    "cv2.putText(numpy_image, \"VGG16: {}, {:.2f}\".format(label_vgg[0][0][1], label_vgg[0][0][2]) , (350, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "cv2.putText(numpy_image, \"MobileNet: {}, {:.2f}\".format(label_mobilenet[0][0][1], label_mobilenet[0][0][2]) , (350, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "cv2.putText(numpy_image, \"Inception: {}, {:.2f}\".format(label_inception[0][0][1], label_inception[0][0][2]) , (350, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "cv2.putText(numpy_image, \"ResNet50: {}, {:.2f}\".format(label_resnet[0][0][1], label_resnet[0][0][2]) , (350, 145), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "numpy_image = cv2.resize(numpy_image, (700,700))\n",
    "cv2.imwrite(\"images/{}_output.jpg\".format(filename.split('/')[-1].split('.')[0]),cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
