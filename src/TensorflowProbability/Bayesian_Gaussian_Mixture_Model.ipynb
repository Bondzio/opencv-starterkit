{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "htW5SiGzeXYm"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HGeUNoteaSm"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ3UDciDVcB5"
   },
   "source": [
    "# Bayesian Gaussian Mixture Model and Hamiltonian MCMC\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lin40yCC6eBo"
   },
   "source": [
    "\n",
    "In this colab we'll explore sampling from the posterior of a Bayesian Gaussian Mixture Model (BGMM) using only TensorFlow Probability primitives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZs1ShikNBK2"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JjokKMbk2hJ"
   },
   "source": [
    "For $k\\in\\{1,\\ldots, K\\}$ mixture components each of dimension $D$, we'd like to model $i\\in\\{1,\\ldots,N\\}$ iid samples using the following Bayesian Gaussian Mixture Model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\theta &\\sim \\text{Dirichlet}(\\text{concentration}=\\alpha_0)\\\\\n",
    "\\mu_k &\\sim \\text{Normal}(\\text{loc}=\\mu_{0k}, \\text{scale}=I_D)\\\\\n",
    "T_k &\\sim \\text{Wishart}(\\text{df}=5, \\text{scale}=I_D)\\\\\n",
    "Z_i &\\sim \\text{Categorical}(\\text{probs}=\\theta)\\\\\n",
    "Y_i &\\sim \\text{Normal}(\\text{loc}=\\mu_{z_i}, \\text{scale}=T_{z_i}^{-1/2})\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iySRABi0qZnQ"
   },
   "source": [
    "Note, the `scale` arguments all have `cholesky` semantics. We use this convention because it is that of TF Distributions (which itself uses this convention in part because it is computationally advantageous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6X_Beihwzyi"
   },
   "source": [
    "Our goal is to generate samples from the posterior:\n",
    "\n",
    "$$p\\left(\\theta, \\{\\mu_k, T_k\\}_{k=1}^K \\Big| \\{y_i\\}_{i=1}^N, \\alpha_0, \\{\\mu_{ok}\\}_{k=1}^K\\right)$$\n",
    "\n",
    "Notice that $\\{Z_i\\}_{i=1}^N$ is not present--we're interested in only those random variables which don't scale with $N$.  (And luckily there's a TF distribution which handles marginalizing out $Z_i$.)\n",
    "\n",
    "It is not possible to directly sample from this distribution owing to a computationally intractable normalization term.\n",
    "\n",
    "[Metropolis-Hastings algorithms](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) are technique for for sampling from intractable-to-normalize distributions.\n",
    "\n",
    "TensorFlow Probability offers a number of MCMC options, including several based on Metropolis-Hastings. In this notebook, we'll use [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)  (`tfp.mcmc.HamiltonianMonteCarlo`). HMC is often a good choice because it can converge rapidly, samples the state space jointly (as opposed to coordinatewise), and leverages one of TF's virtues: automatic differentiation. That said, sampling from a BGMM posterior might actually be better done by other approaches, e.g., [Gibb's sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uswTWdgNu46j"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovNsKD-OEUzR"
   },
   "outputs": [],
   "source": [
    "def session_options(enable_gpu_ram_resizing=True):\n",
    "  \"\"\"Convenience function which sets common `tf.Session` options.\"\"\"\n",
    "  config = tf.ConfigProto()\n",
    "  config.log_device_placement = True\n",
    "  if enable_gpu_ram_resizing:\n",
    "    # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "    # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "    config.gpu_options.allow_growth = True\n",
    "  return config\n",
    "\n",
    "def reset_sess(config=None):\n",
    "  \"\"\"Convenience function to create the TF graph and session, or reset them.\"\"\"\n",
    "  if config is None:\n",
    "    config = session_options()\n",
    "  tf.reset_default_graph()\n",
    "  global sess\n",
    "  try:\n",
    "    sess.close()\n",
    "  except:\n",
    "    pass\n",
    "  sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "reset_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uj9uHZN2yUqz"
   },
   "source": [
    "Before actually building the model, we'll need to define a new type of distribution.  From the model specification above, its clear we're parameterizing the MVN with an inverse covariance matrix, i.e.,  [precision matrix](https://en.wikipedia.org/wiki/Precision_(statistics%29).  To accomplish this in TF,  we'll need to roll out our `Bijector`.  This `Bijector` will use the forward transformation:\n",
    "\n",
    "- `Y =`  [`tf.matrix_triangular_solve`](https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve)`(tf.matrix_transpose(chol_precision_tril), X, adjoint=True) + loc`.\n",
    "\n",
    "And the `log_prob` calculation is just the inverse, i.e.:\n",
    "\n",
    "- `X =` [`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/matmul)`(chol_precision_tril, X - loc, adjoint_a=True)`.\n",
    "\n",
    "Since all we need for HMC is `log_prob`, this means we avoid ever calling `tf.matrix_triangular_solve` (as would be the case for `tfd.MultivariateNormalTriL`). This is advantageous since `tf.matmul` is usually faster owing to better cache locality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nc4yy6vW-lC_"
   },
   "outputs": [],
   "source": [
    "class MVNCholPrecisionTriL(tfd.TransformedDistribution):\n",
    "  \"\"\"MVN from loc and (Cholesky) precision matrix.\"\"\"\n",
    "\n",
    "  def __init__(self, loc, chol_precision_tril, name=None):\n",
    "    super(MVNCholPrecisionTriL, self).__init__(\n",
    "        distribution=tfd.Independent(tfd.Normal(tf.zeros_like(loc),\n",
    "                                                scale=tf.ones_like(loc)),\n",
    "                                     reinterpreted_batch_ndims=1),\n",
    "        bijector=tfb.Chain([\n",
    "            tfb.Affine(shift=loc),\n",
    "            tfb.Invert(tfb.Affine(scale_tril=chol_precision_tril,\n",
    "                                  adjoint=True)),\n",
    "        ]),\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDOkWhDQg4ZG"
   },
   "source": [
    "The `tfd.Independent` distribution turns independent draws of one distribution, into a multivariate distribution with statistically independent coordinates. In terms of computing `log_prob`, this \"meta-distribution\" manifests as a simple sum over the event dimension(s).\n",
    "\n",
    "Also notice that we took the `adjoint` (\"transpose\") of the scale matrix. This is because if precision is inverse covariance, i.e., $P=C^{-1}$ and if $C=AA^\\top$, then $P=BB^{\\top}$ where $B=A^{-\\top}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pfkc8cmhh2Qz"
   },
   "source": [
    "Since this distribution is kind of tricky, let's quickly verify that our `MVNCholPrecisionTriL` works as we think it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1538760458123,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "GhqbjwlIh1Vn",
    "outputId": "d9c48cc8-5dd2-45c9-9ccc-3be0da3619ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean: [ 1. -1.]\n",
      "sample mean: [ 1.0002806 -1.000105 ]\n",
      "true cov:\n",
      " [[ 1.0625   -0.03125 ]\n",
      " [-0.03125   0.015625]]\n",
      "sample cov:\n",
      " [[ 1.0641283  -0.03126172]\n",
      " [-0.03126172  0.01559312]]\n"
     ]
    }
   ],
   "source": [
    "def compute_sample_stats(d, seed=42, n=int(1e6)):\n",
    "  x = d.sample(n, seed=seed)\n",
    "  sample_mean = tf.reduce_mean(x, axis=0, keepdims=True)\n",
    "  s = x - sample_mean\n",
    "  sample_cov = tf.matmul(s, s, adjoint_a=True) / tf.cast(n, s.dtype)\n",
    "  sample_scale = tf.cholesky(sample_cov)\n",
    "  sample_mean = sample_mean[0]\n",
    "  return [\n",
    "      sample_mean,\n",
    "      sample_cov,\n",
    "      sample_scale,\n",
    "  ]\n",
    "\n",
    "dtype = np.float32\n",
    "true_loc = np.array([1., -1.], dtype=dtype)\n",
    "true_chol_precision = np.array([[1., 0.],\n",
    "                                [2., 8.]],\n",
    "                               dtype=dtype)\n",
    "true_precision = np.matmul(true_chol_precision, true_chol_precision.T)\n",
    "true_cov = np.linalg.inv(true_precision)\n",
    "\n",
    "d = MVNCholPrecisionTriL(\n",
    "    loc=true_loc,\n",
    "    chol_precision_tril=true_chol_precision)\n",
    "\n",
    "[\n",
    "    sample_mean_,\n",
    "    sample_cov_,\n",
    "    sample_scale_,\n",
    "] = sess.run(compute_sample_stats(d))\n",
    "\n",
    "print('true mean:', true_loc)\n",
    "print('sample mean:', sample_mean_)\n",
    "print('true cov:\\n', true_cov)\n",
    "print('sample cov:\\n', sample_cov_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N60z8scN1v6E"
   },
   "source": [
    "Since the sample mean and covariance are close to the true mean and covariance, it seems like the distribution is correctly implemented. Now, we'll use `MVNCholPrecisionTriL` and  stock`tfp.distributions` to specify the BGMM prior random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhzxySDjL2-S"
   },
   "outputs": [],
   "source": [
    "dtype = np.float32\n",
    "dims = 2\n",
    "components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAOmHhZ7LzDQ"
   },
   "outputs": [],
   "source": [
    "rv_mix_probs = tfd.Dirichlet(\n",
    "    concentration=np.ones(components, dtype) / 10.,\n",
    "    name='rv_mix_probs')\n",
    "\n",
    "rv_loc = tfd.Independent(\n",
    "    tfd.Normal(\n",
    "        loc=np.stack([\n",
    "            -np.ones(dims, dtype),\n",
    "            np.zeros(dims, dtype),\n",
    "            np.ones(dims, dtype),\n",
    "        ]),\n",
    "        scale=tf.ones([components, dims], dtype)),\n",
    "    reinterpreted_batch_ndims=1,\n",
    "    name='rv_loc')\n",
    "\n",
    "rv_precision = tfd.Wishart(\n",
    "    df=5,\n",
    "    scale_tril=np.stack([np.eye(dims, dtype=dtype)]*components),\n",
    "    input_output_cholesky=True,\n",
    "    name='rv_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1538760469863,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "KSTp8aAIAv0O",
    "outputId": "9b18cf98-8514-471b-a56f-429be5b30d3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Dirichlet(\"rv_mix_probs/\", batch_shape=(), event_shape=(3,), dtype=float32)\n",
      "tfp.distributions.Independent(\"rv_loc/\", batch_shape=(3,), event_shape=(2,), dtype=float32)\n",
      "tfp.distributions.Wishart(\"rv_precision/\", batch_shape=(3,), event_shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(rv_mix_probs)\n",
    "print(rv_loc)\n",
    "print(rv_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZOG0OR815Nr"
   },
   "source": [
    "Using the three random variables defined above, we can now specify the joint log probability function. To do this we'll use `tfd.MixtureSameFamily` to automatically integrate out the categorical $\\{Z_i\\}_{i=1}^N$ draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpLnRJr2TXYD"
   },
   "outputs": [],
   "source": [
    "def joint_log_prob(observations, mix_probs, loc, chol_precision):\n",
    "  \"\"\"BGMM with priors: loc=Normal, precision=Inverse-Wishart, mix=Dirichlet.\n",
    "\n",
    "  Args:\n",
    "    observations: `[n, d]`-shaped `Tensor` representing Bayesian Gaussian\n",
    "      Mixture model draws. Each sample is a length-`d` vector.\n",
    "    mix_probs: `[K]`-shaped `Tensor` representing random draw from\n",
    "      `SoftmaxInverse(Dirichlet)` prior.\n",
    "    loc: `[K, d]`-shaped `Tensor` representing the location parameter of the\n",
    "      `K` components.\n",
    "    chol_precision: `[K, d, d]`-shaped `Tensor` representing `K` lower\n",
    "      triangular `cholesky(Precision)` matrices, each being sampled from\n",
    "      a Wishart distribution.\n",
    "\n",
    "  Returns:\n",
    "    log_prob: `Tensor` representing joint log-density over all inputs.\n",
    "  \"\"\"\n",
    "  rv_observations = tfd.MixtureSameFamily(\n",
    "      mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
    "      components_distribution=MVNCholPrecisionTriL(\n",
    "          loc=loc,\n",
    "          chol_precision_tril=chol_precision))\n",
    "  log_prob_parts = [\n",
    "      rv_observations.log_prob(observations), # Sum over samples.\n",
    "      rv_mix_probs.log_prob(mix_probs)[..., tf.newaxis],\n",
    "      rv_loc.log_prob(loc),                   # Sum over components.\n",
    "      rv_precision.log_prob(chol_precision),  # Sum over components.\n",
    "  ]\n",
    "  sum_log_prob = tf.reduce_sum(tf.concat(log_prob_parts, axis=-1), axis=-1)\n",
    "  # Note: for easy debugging, uncomment the following:\n",
    "  # sum_log_prob = tf.Print(sum_log_prob, log_prob_parts)\n",
    "  return sum_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1idLJazkGC"
   },
   "source": [
    "Notice that this function internally defines a new random variable. This is necessary since the `observations` RV depends on samples from the RVs defined further above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jTMXdymV1QJ"
   },
   "source": [
    "## Generate \"Training\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rl4brz3G3pS7"
   },
   "source": [
    "For this demo, we'll sample some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AJZAtwXV8RQ"
   },
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "true_loc = np.array([[-2, -2],\n",
    "                     [0, 0],\n",
    "                     [2, 2]], dtype)\n",
    "random = np.random.RandomState(seed=42)\n",
    "\n",
    "true_hidden_component = random.randint(0, components, num_samples)\n",
    "observations = (true_loc[true_hidden_component] +\n",
    "                random.randn(num_samples, dims).astype(dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVOvMh7MV37A"
   },
   "source": [
    "## Bayesian Inference using HMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdN3iKFT32Jp"
   },
   "source": [
    "Now that we've used TFD to specify our model and obtained some observed data, we have all the necessary pieces to run HMC.\n",
    "\n",
    "To do this, we'll use a [partial application](https://en.wikipedia.org/wiki/Partial_application) to \"pin down\" the things we don't want to sample. In this case that means we need only pin down `observations`. (The hyper-parameters are already baked in to the prior distributions and not part of the `joint_log_prob` function signature.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVoaDFSf7L_j"
   },
   "outputs": [],
   "source": [
    "unnormalized_posterior_log_prob = functools.partial(joint_log_prob, observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0OMIWIYeMmQ"
   },
   "outputs": [],
   "source": [
    "initial_state = [\n",
    "    tf.fill([components],\n",
    "            value=np.array(1. / components, dtype),\n",
    "            name='mix_probs'),\n",
    "    tf.constant(np.array([[-2, -2],\n",
    "                          [0, 0],\n",
    "                          [2, 2]], dtype),\n",
    "                name='loc'),\n",
    "    tf.eye(dims, batch_shape=[components], dtype=dtype, name='chol_precision'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVpiT3LLyfcO"
   },
   "source": [
    "### Unconstrained Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS8XOsxiyiBV"
   },
   "source": [
    "Hamiltonian Monte Carlo (HMC) requires the target log-probability function be differentiable with respect to its arguments.  Furthermore, HMC can exhibit dramatically higher statistical efficiency if the state-space is unconstrained.\n",
    "\n",
    "This means we'll have to work out two main issues when sampling from the BGMM posterior:\n",
    "\n",
    "1. $\\theta$ represents a discrete probability vector, i.e., must be such that $\\sum_{k=1}^K \\theta_k = 1$ and $\\theta_k>0$.\n",
    "2. $T_k$ represents an inverse covariance matrix, i.e., must be such that $T_k \\succ 0$, i.e., is [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt9SXJzO0Cks"
   },
   "source": [
    "To address this requirement we'll need to:\n",
    "\n",
    "1. transform the constrained variables to an unconstrained space\n",
    "2. run the MCMC in unconstrained space\n",
    "3. transform the unconstrained variables back to the constrained space.\n",
    "\n",
    "As with `MVNCholPrecisionTriL`, we'll use [`Bijector`s](https://www.tensorflow.org/api_docs/python/tf/distributions/bijectors/Bijector) to transform random variables to unconstrained space.\n",
    "\n",
    "- The [`Dirichlet`](https://en.wikipedia.org/wiki/Dirichlet_distribution) is transformed to unconstrained space via the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "\n",
    "- Our precision random variable is a distribution over postive semidefinite matrices. To unconstrain these we'll use the `FillTriangular` and `TransformDiagonal` bijectors.  These convert vectors to lower-triangular matrices and ensure the diagonal is positive. The former is useful because it enables sampling only $d(d+1)/2$ floats rather than $d^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_atEQrDR7JvG"
   },
   "outputs": [],
   "source": [
    "unconstraining_bijectors = [\n",
    "    tfb.SoftmaxCentered(),\n",
    "    tfb.Identity(),\n",
    "    tfb.Chain([\n",
    "        tfb.TransformDiagonal(tfb.Softplus()),\n",
    "        tfb.FillTriangular(),\n",
    "    ])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0zq6QJJ-NSPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_probability/python/bijectors/softmax_centered.py:158: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "[mix_probs, loc, chol_precision], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=2000,\n",
    "    num_burnin_steps=500,\n",
    "    current_state=initial_state,\n",
    "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "            step_size=0.065,\n",
    "            num_leapfrog_steps=5),\n",
    "        bijector=unconstraining_bijectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ceX1A3-ZFiN"
   },
   "outputs": [],
   "source": [
    "acceptance_rate = tf.reduce_mean(tf.to_float(kernel_results.inner_results.is_accepted))\n",
    "mean_mix_probs = tf.reduce_mean(mix_probs, axis=0)\n",
    "mean_loc = tf.reduce_mean(loc, axis=0)\n",
    "mean_chol_precision = tf.reduce_mean(chol_precision, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmpTFZcVmByb"
   },
   "source": [
    "Note: through trial-and-error we've predetermined the `step_size` and `num_leapfrog_steps` to approximately achieve an [asymptotically optimal rate of 0.651](https://arxiv.org/abs/1001.4460). For a technique to do this automatically, see the examples section in `help(tfp.mcmc.HamiltonianMonteCarlo)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLEz96mg6fpZ"
   },
   "source": [
    "We'll now execute the chain and print the posterior means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3B2yJWVmNcrm"
   },
   "outputs": [],
   "source": [
    "[\n",
    "    acceptance_rate_,\n",
    "    mean_mix_probs_,\n",
    "    mean_loc_,\n",
    "    mean_chol_precision_,\n",
    "    mix_probs_,\n",
    "    loc_,\n",
    "    chol_precision_,\n",
    "] = sess.run([\n",
    "    acceptance_rate,\n",
    "    mean_mix_probs,\n",
    "    mean_loc,\n",
    "    mean_chol_precision,\n",
    "    mix_probs,\n",
    "    loc,\n",
    "    chol_precision,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1538760907401,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "bqJ6RSJxegC6",
    "outputId": "25a86519-852f-4368-f8d4-2914ff1db494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    acceptance_rate: 0.658\n",
      "      avg mix probs: [0.3919248  0.24706787 0.36100644]\n",
      "\n",
      "            avg loc:\n",
      " [[-1.8643277  -1.7922589 ]\n",
      " [-0.01199361  0.01205728]\n",
      " [ 1.8804541   1.9134417 ]]\n",
      "\n",
      "avg chol(precision):\n",
      " [[[ 1.0032482   0.        ]\n",
      "  [-0.05567271  0.9714632 ]]\n",
      "\n",
      " [[ 1.2531575   0.        ]\n",
      "  [ 0.26007688  1.0437504 ]]\n",
      "\n",
      " [[ 0.9831322   0.        ]\n",
      "  [-0.11540155  0.96545523]]]\n"
     ]
    }
   ],
   "source": [
    "print('    acceptance_rate:', acceptance_rate_)\n",
    "print('      avg mix probs:', mean_mix_probs_)\n",
    "print('\\n            avg loc:\\n', mean_loc_)\n",
    "print('\\navg chol(precision):\\n', mean_chol_precision_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11557,
     "status": "ok",
     "timestamp": 1538760926241,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 420
    },
    "id": "zFOU0j9kPdUy",
    "outputId": "187b368d-0fb7-4e49-88b5-e215f4a27d9d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.kdeplot(loc_[:,0,0], loc_[:,0,1], shade=True)\n",
    "ax = sns.kdeplot(loc_[:,1,0], loc_[:,1,1], shade=True)\n",
    "ax = sns.kdeplot(loc_[:,2,0], loc_[:,2,1], shade=True)\n",
    "plt.title('KDE of loc draws');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NmfNIM1c6mwc"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8LeIeMn6ot4"
   },
   "source": [
    "This simple colab demonstrated how TensorFlow Probability primitives can be used to build hierarchical Bayesian mixture models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bayesian_Gaussian_Mixture_Model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
